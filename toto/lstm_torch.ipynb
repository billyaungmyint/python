{"cells":[{"source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nimport random\n\n# get the past winning numbers from the web\nurl = 'https://en.lottolyzer.com/history-export-csv/singapore/toto/ToTo.csv'\n# read the url\ndata_csv = pd.read_csv(url)\n\ndef rnn(isize , hsize , osize, lsize):\n    # from the data , choose the specific columns from there , winning numbers + additional number\n    # winning_number = data_csv[['Winning Number 1', '2', '3', '4', '5', '6', 'Additional Number ']]\n    # from the data , choose the specific columns from there , winning numbers \n    winning_number = data_csv[['Winning Number 1', '2', '3', '4', '5', '6']]\n    # rename the winning number and additional number columns to 1 and 7 respectively\n    # winning_number = winning_number.rename({'Winning Number 1': '1', 'Additional Number ': '7'}, axis=1)\n    # rename the winning number \n    winning_number = winning_number.rename({'Winning Number 1': '1'}, axis=1)\n\n    # make a new list of winning numbers of the most recent 10 winning numbers \n    winning_list = winning_number.iloc[0:lsize].values.tolist()\n\n\n    # Convert data to a numpy array\n    data = np.array(winning_list, dtype=np.float32)\n\n    # Prepare input and output data\n    X = data[:-1]  # Input sequences\n    y = data[1:]   # Output sequences\n\n    # Convert data to PyTorch tensors\n    X = torch.tensor(X)\n    y = torch.tensor(y)\n\n    # Define the RNN model\n    class RNN(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(RNN, self).__init__()\n            self.hidden_size = hidden_size\n            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n            self.fc = nn.Linear(hidden_size, output_size)\n\n        def forward(self, x):\n            out, _ = self.rnn(x)\n            out = self.fc(out)\n            return out\n\n    input_size = isize # input size either with additional number then 7 or without then 6\n    hidden_size = hsize  # You can adjust this as needed\n    output_size = osize\n\n    model = RNN(input_size, hidden_size, output_size)\n\n    # Define loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Training loop\n    num_epochs = 1000\n\n    for epoch in range(num_epochs):\n        optimizer.zero_grad()\n        outputs = model(X.unsqueeze(0))  # Add a batch dimension\n        loss = criterion(outputs, y.unsqueeze(0))  # Add a batch dimension\n        loss.backward()\n        optimizer.step()\n\n        # if (epoch + 1) % 100 == 0:\n        #     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n    # Predict the next list\n    with torch.no_grad():\n        last_sequence = X[-1].unsqueeze(0).unsqueeze(0)  # Add batch and sequence dimensions\n        next_list = model(last_sequence)\n        next_list = next_list.squeeze().numpy()\n\n    print(f\"Predicted Next List according to RNN based on hidden size {hsize} :\")\n    print([int(x) for x in next_list])\n\ndef lstm(isize , hsize , osize , lsize):\n    \n    # from the data , choose the specific columns from there , winning numbers + additional number\n    # winning_number = data_csv[['Winning Number 1', '2', '3', '4', '5', '6', 'Additional Number ']]\n    # from the data , choose the specific columns from there , winning numbers \n    winning_number = data_csv[['Winning Number 1', '2', '3', '4', '5', '6']]\n    # rename the winning number and additional number columns to 1 and 7 respectively\n    # winning_number = winning_number.rename({'Winning Number 1': '1', 'Additional Number ': '7'}, axis=1)\n    # rename the winning number \n    winning_number = winning_number.rename({'Winning Number 1': '1'}, axis=1)\n\n    # make a new list of winning numbers of the most recent 10 winning numbers \n    winning_list = winning_number.iloc[0:lsize].values.tolist()\n\n\n    # Convert data to a numpy array\n    data = np.array(winning_list, dtype=np.float32)\n    \n    # Prepare input and output data\n    X = data[:-1]  # Input sequences\n    y = data[1:]   # Output sequences\n\n    # Convert data to PyTorch tensors\n    X = torch.tensor(X)\n    y = torch.tensor(y)\n\n    # Define the LSTM model\n    class LSTM(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(LSTM, self).__init__()\n            self.hidden_size = hidden_size\n            self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n            self.fc = nn.Linear(hidden_size, output_size)\n\n        def forward(self, x):\n            out, _ = self.lstm(x)\n            out = self.fc(out)\n            return out\n\n    input_size = isize\n    hidden_size = hsize # You can adjust this as needed\n    output_size = osize\n\n    model = LSTM(input_size, hidden_size, output_size)\n\n    # Define loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Training loop\n    num_epochs = 1000\n\n    for epoch in range(num_epochs):\n        optimizer.zero_grad()\n        outputs = model(X.unsqueeze(0))  # Add a batch dimension\n        loss = criterion(outputs, y.unsqueeze(0))  # Add a batch dimension\n        loss.backward()\n        optimizer.step()\n\n        # if (epoch + 1) % 100 == 0:\n        #     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n    # Predict the next list\n    with torch.no_grad():\n        last_sequence = X[-1].unsqueeze(0).unsqueeze(0)  # Add batch and sequence dimensions\n        next_list = model(last_sequence)\n        next_list = next_list.squeeze().numpy()\n\n    print(f\"Predicted Next List according to LSTM based on hidden size {hsize} : \")\n    print([int(x) for x in next_list])\n\nprint(\"most recent 5 numbers\")\nlstm(6,50,6,5)\nlstm(6,100,6,5)\nrnn(6,50,6,5)\nrnn(6,100,6,5)\nprint(\"most recent 10 numbers\")\nlstm(6,50,6,10)\nlstm(6,100,6,10)\nrnn(6,50,6,10)\nrnn(6,100,6,10)\nprint(\"most recent 20 numbers\")\nlstm(6,50,6,20)\nlstm(6,100,6,20)\nrnn(6,50,6,20)\nrnn(6,100,6,20)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":544,"type":"stream"}}},"cell_type":"code","id":"dadcf10b-a19e-442d-b128-d67498a25976","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"most recent 5 numbers\nPredicted Next List according to LSTM based on hidden size 50 : \n[2, 9, 17, 19, 28, 30]\nPredicted Next List according to LSTM based on hidden size 100 : \n[2, 9, 17, 19, 31, 35]\nPredicted Next List according to RNN based on hidden size 50 :\n[2, 11, 20, 23, 34, 37]\nPredicted Next List according to RNN based on hidden size 100 :\n[2, 11, 20, 23, 39, 43]\nmost recent 10 numbers\nPredicted Next List according to LSTM based on hidden size 50 : \n[4, 9, 16, 20, 27, 30]\nPredicted Next List according to LSTM based on hidden size 100 : \n[3, 9, 16, 20, 30, 35]\nPredicted Next List according to RNN based on hidden size 50 :\n[5, 12, 20, 25, 33, 36]\nPredicted Next List according to RNN based on hidden size 100 :\n[5, 12, 20, 26, 37, 44]\nmost recent 20 numbers\nPredicted Next List according to LSTM based on hidden size 50 : \n[4, 11, 17, 21, 26, 29]\nPredicted Next List according to LSTM based on hidden size 100 : \n[4, 11, 17, 21, 28, 34]\nPredicted Next List according to RNN based on hidden size 50 :\n[5, 14, 22, 27, 32, 36]\nPredicted Next List according to RNN based on hidden size 100 :\n[5, 13, 21, 27, 35, 43]\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}